{
  "annotations": {
    "list": [
      {
        "builtIn": 1,
        "datasource": {
          "type": "grafana",
          "uid": "-- Grafana --"
        },
        "enable": true,
        "hide": true,
        "iconColor": "rgba(0, 211, 255, 1)",
        "name": "Annotations & Alerts",
        "type": "dashboard"
      }
    ]
  },
  "description": "LLM request metrics, token usage, cost analysis, latency, and traces for the Archestra platform",
  "editable": true,
  "fiscalYearStartMonth": 0,
  "graphTooltip": 1,
  "id": 2,
  "links": [
    {
      "asDropdown": false,
      "icon": "dashboard",
      "includeVars": true,
      "keepTime": true,
      "tags": [],
      "targetBlank": false,
      "title": "MCP Monitoring",
      "tooltip": "",
      "type": "link",
      "url": "/d/archestra-mcp-monitoring/archestra-mcp-monitoring"
    },
    {
      "asDropdown": false,
      "icon": "dashboard",
      "includeVars": true,
      "keepTime": true,
      "tags": [],
      "targetBlank": false,
      "title": "Agent Sessions",
      "tooltip": "",
      "type": "link",
      "url": "/d/archestra-agent-sessions/archestra-agent-sessions"
    },
    {
      "asDropdown": false,
      "icon": "dashboard",
      "includeVars": true,
      "keepTime": true,
      "tags": [],
      "targetBlank": false,
      "title": "Application Metrics",
      "tooltip": "",
      "type": "link",
      "url": "/d/archestra-application-metrics/archestra-application-metrics"
    },
    {
      "asDropdown": false,
      "icon": "doc",
      "includeVars": false,
      "keepTime": false,
      "tags": [],
      "targetBlank": true,
      "title": "Documentation",
      "tooltip": "Archestra Documentation",
      "type": "link",
      "url": "https://archestra.ai/docs/platform-observability"
    }
  ],
  "panels": [
    {
      "fieldConfig": {
        "defaults": {},
        "overrides": []
      },
      "gridPos": {
        "h": 4,
        "w": 3,
        "x": 0,
        "y": 0
      },
      "id": 61,
      "options": {
        "code": {
          "language": "plaintext",
          "showLineNumbers": false,
          "showMiniMap": false
        },
        "content": "<div style=\"display: flex; align-items: center; justify-content: center; height: 100%;\"><a href=\"https://archestra.ai\" target=\"_blank\"><img src=\"https://archestra.ai/logo.png\" alt=\"Archestra\" style=\"height: 100%;\" /></a></div>",
        "mode": "html"
      },
      "pluginVersion": "12.3.2",
      "title": "",
      "transparent": true,
      "type": "text"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "${metrics_datasource}"
      },
      "description": "This panel displays the current LLM request rate showing the number of GenAI requests per second across all services. This metric helps monitor system load and demand patterns for capacity planning.",
      "fieldConfig": {
        "defaults": {
          "color": {
            "fixedColor": "blue",
            "mode": "palette-classic-by-name"
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "red",
                "value": 0
              },
              {
                "color": "#EAB839",
                "value": 10
              },
              {
                "color": "#6ED0E0",
                "value": 100
              }
            ]
          },
          "unit": "reqps"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 4,
        "w": 5,
        "x": 4,
        "y": 0
      },
      "id": 22,
      "options": {
        "colorMode": "background",
        "graphMode": "area",
        "justifyMode": "auto",
        "orientation": "auto",
        "percentChangeColorMode": "standard",
        "reduceOptions": {
          "calcs": ["lastNotNull"],
          "fields": "",
          "values": false
        },
        "showPercentChange": true,
        "textMode": "value_and_name",
        "wideLayout": true
      },
      "pluginVersion": "12.3.2",
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "${metrics_datasource}"
          },
          "disableTextWrap": false,
          "editorMode": "builder",
          "expr": "sum(rate(llm_request_duration_seconds_count{agent_name=~\"$agent_name\", provider=~\"$provider\", model=~\"$model\"}[$__rate_interval]))",
          "fullMetaSearch": false,
          "includeNullMetadata": true,
          "instant": false,
          "legendFormat": "Request Rate",
          "range": true,
          "refId": "A",
          "useBackend": false
        }
      ],
      "title": "",
      "transparent": true,
      "type": "stat"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "${metrics_datasource}"
      },
      "description": "This panel displays the total number of tokens consumed by GenAI requests, providing a direct measure of usage volume. Monitoring this helps in assessing demand on GenAI services and guiding resource allocation strategies.",
      "fieldConfig": {
        "defaults": {
          "color": {
            "fixedColor": "purple",
            "mode": "shades"
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": 0
              },
              {
                "color": "red",
                "value": 80
              }
            ]
          },
          "unit": "short"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 4,
        "w": 5,
        "x": 9,
        "y": 0
      },
      "id": 3,
      "options": {
        "colorMode": "background",
        "graphMode": "area",
        "justifyMode": "auto",
        "orientation": "auto",
        "percentChangeColorMode": "standard",
        "reduceOptions": {
          "calcs": ["lastNotNull"],
          "fields": "",
          "values": false
        },
        "showPercentChange": true,
        "textMode": "auto",
        "wideLayout": true
      },
      "pluginVersion": "12.3.2",
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "${metrics_datasource}"
          },
          "disableTextWrap": false,
          "editorMode": "code",
          "expr": "sum(llm_tokens_total{agent_name=~\"$agent_name\", provider=~\"$provider\", model=~\"$model\"})",
          "fullMetaSearch": false,
          "includeNullMetadata": true,
          "instant": false,
          "legendFormat": "Total Usage Tokens",
          "range": true,
          "refId": "A",
          "useBackend": false
        }
      ],
      "title": "",
      "transparent": true,
      "type": "stat"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "${metrics_datasource}"
      },
      "description": "This panel displays the average cost per use of the GenAI models and related services. It provides insights into the cost-effectiveness of interactions with GenAI, helping to identify trends in expense per operation.",
      "fieldConfig": {
        "defaults": {
          "color": {
            "fixedColor": "blue",
            "mode": "shades"
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": 0
              },
              {
                "color": "#EAB839",
                "value": 0.5
              },
              {
                "color": "red",
                "value": 1
              }
            ]
          },
          "unit": "currencyUSD"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 4,
        "w": 5,
        "x": 14,
        "y": 0
      },
      "id": 5,
      "options": {
        "colorMode": "background",
        "graphMode": "area",
        "justifyMode": "auto",
        "orientation": "auto",
        "percentChangeColorMode": "inverted",
        "reduceOptions": {
          "calcs": ["lastNotNull"],
          "fields": "",
          "values": false
        },
        "showPercentChange": true,
        "textMode": "auto",
        "wideLayout": true
      },
      "pluginVersion": "12.3.2",
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "${metrics_datasource}"
          },
          "disableTextWrap": false,
          "editorMode": "code",
          "expr": "(sum(\n  increase(\n    llm_cost_total{agent_name=~\"$agent_name\", provider=~\"$provider\", model=~\"$model\"}[$__range]\n  )\n)\n/\nsum(\n  increase(\n    llm_request_duration_seconds_count{agent_name=~\"$agent_name\", provider=~\"$provider\", model=~\"$model\"}[$__range]\n  )\n)) / 10000",
          "fullMetaSearch": false,
          "includeNullMetadata": true,
          "instant": false,
          "legendFormat": "Avg Usage Cost",
          "range": true,
          "refId": "A",
          "useBackend": false
        }
      ],
      "title": "",
      "transparent": true,
      "type": "stat"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "${metrics_datasource}"
      },
      "description": "This panel displays the total cost incurred from using GenAI models. It reflects the financial impact of operational activities, offering insights into budgetary allocation and efficiency. Tracking this helps in effective cost management and financial planning for GenAI usage.",
      "fieldConfig": {
        "defaults": {
          "color": {
            "fixedColor": "blue",
            "mode": "shades"
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": 0
              },
              {
                "color": "red",
                "value": 80
              }
            ]
          },
          "unit": "currencyUSD"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 4,
        "w": 5,
        "x": 19,
        "y": 0
      },
      "id": 2,
      "options": {
        "colorMode": "background",
        "graphMode": "area",
        "justifyMode": "auto",
        "orientation": "auto",
        "percentChangeColorMode": "standard",
        "reduceOptions": {
          "calcs": ["lastNotNull"],
          "fields": "",
          "values": false
        },
        "showPercentChange": true,
        "textMode": "auto",
        "wideLayout": true
      },
      "pluginVersion": "12.3.2",
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "${metrics_datasource}"
          },
          "disableTextWrap": false,
          "editorMode": "code",
          "expr": "sum(increase(llm_cost_total{agent_name=~\"$agent_name\", provider=~\"$provider\", model=~\"$model\"}[$__range])) / 10000",
          "fullMetaSearch": false,
          "includeNullMetadata": true,
          "instant": false,
          "legendFormat": "Total Usage Cost",
          "range": true,
          "refId": "A",
          "useBackend": false
        }
      ],
      "title": "",
      "transparent": true,
      "type": "stat"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "${metrics_datasource}"
      },
      "description": "Request volume distribution across different GenAI systems",
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisBorderShow": false,
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "barWidthFactor": 0.6,
            "drawStyle": "line",
            "fillOpacity": 30,
            "gradientMode": "none",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "insertNulls": false,
            "lineInterpolation": "linear",
            "lineWidth": 2,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "auto",
            "showValues": false,
            "spanNulls": false,
            "stacking": {
              "group": "A",
              "mode": "normal"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "mappings": [],
          "min": 0,
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": 0
              },
              {
                "color": "red",
                "value": 80
              }
            ]
          },
          "unit": "reqps"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 4,
        "w": 6,
        "x": 0,
        "y": 4
      },
      "id": 54,
      "options": {
        "legend": {
          "calcs": [],
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": false
        },
        "tooltip": {
          "hideZeros": false,
          "mode": "multi",
          "sort": "none"
        }
      },
      "pluginVersion": "12.3.2",
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "${metrics_datasource}"
          },
          "expr": "sum(rate(llm_request_duration_seconds_count{agent_name=~\"$agent_name\", provider=~\"$provider\", model=~\"$model\"}[$__rate_interval])) by (provider)",
          "legendFormat": "{{provider}}",
          "range": true,
          "refId": "A",
          "exemplar": true
        }
      ],
      "title": "Request Volume by Provider",
      "transparent": true,
      "type": "timeseries"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "${metrics_datasource}"
      },
      "description": "Hourly cost trends to identify spending patterns",
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisBorderShow": false,
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "barWidthFactor": 0.6,
            "drawStyle": "line",
            "fillOpacity": 20,
            "gradientMode": "none",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "insertNulls": false,
            "lineInterpolation": "linear",
            "lineWidth": 2,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "auto",
            "showValues": false,
            "spanNulls": false,
            "stacking": {
              "group": "A",
              "mode": "none"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "mappings": [],
          "min": 0,
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": 0
              },
              {
                "color": "red",
                "value": 80
              }
            ]
          },
          "unit": "currencyUSD"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 4,
        "w": 6,
        "x": 6,
        "y": 4
      },
      "id": 55,
      "options": {
        "legend": {
          "calcs": [],
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": false
        },
        "tooltip": {
          "hideZeros": true,
          "mode": "multi",
          "sort": "none"
        }
      },
      "pluginVersion": "12.3.2",
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "${metrics_datasource}"
          },
          "editorMode": "code",
          "expr": "increase(llm_cost_total{agent_name=~\"$agent_name\", provider=~\"$provider\", model=~\"$model\"}[1h]) / 1000",
          "legendFormat": "{{provider}}",
          "range": true,
          "refId": "A",
          "exemplar": true
        }
      ],
      "title": "Cost Trend Analysis",
      "transparent": true,
      "type": "timeseries"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "${metrics_datasource}"
      },
      "description": "Percentage of successful GenAI requests over time",
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisBorderShow": false,
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "barWidthFactor": 0.6,
            "drawStyle": "line",
            "fillOpacity": 20,
            "gradientMode": "none",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "insertNulls": false,
            "lineInterpolation": "linear",
            "lineWidth": 2,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "auto",
            "showValues": false,
            "spanNulls": false,
            "stacking": {
              "group": "A",
              "mode": "none"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "mappings": [],
          "min": 0,
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": 0
              },
              {
                "color": "red",
                "value": 80
              }
            ]
          },
          "unit": "reqps"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 4,
        "w": 6,
        "x": 12,
        "y": 4
      },
      "id": 56,
      "options": {
        "legend": {
          "calcs": [],
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "hideZeros": false,
          "mode": "multi",
          "sort": "none"
        }
      },
      "pluginVersion": "12.3.2",
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "${metrics_datasource}"
          },
          "expr": "sum(rate(llm_request_duration_seconds_count{agent_name=~\"$agent_name\", provider=~\"$provider\", model=~\"$model\"}[$__rate_interval]))",
          "legendFormat": "Total Requests",
          "range": true,
          "refId": "A",
          "exemplar": true
        }
      ],
      "title": "Request Success Rate",
      "transparent": true,
      "type": "timeseries"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "${metrics_datasource}"
      },
      "description": "Rate of tool calls blocked by tool invocation policies, grouped by agent name",
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisBorderShow": false,
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "barWidthFactor": 0.6,
            "drawStyle": "line",
            "fillOpacity": 10,
            "gradientMode": "none",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "insertNulls": false,
            "lineInterpolation": "linear",
            "lineWidth": 1,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "auto",
            "showValues": false,
            "spanNulls": false,
            "stacking": {
              "group": "A",
              "mode": "normal"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": 0
              },
              {
                "color": "red",
                "value": 1
              }
            ]
          },
          "unit": "cps"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 4,
        "w": 6,
        "x": 18,
        "y": 4
      },
      "id": 63,
      "options": {
        "legend": {
          "calcs": [],
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "hideZeros": true,
          "mode": "multi",
          "sort": "none"
        }
      },
      "pluginVersion": "12.3.2",
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "${metrics_datasource}"
          },
          "editorMode": "code",
          "expr": "rate(llm_blocked_tools_total{agent_name=~\"$agent_name\", provider=~\"$provider\", model=~\"$model\"}[$__rate_interval])",
          "legendFormat": "{{agent_name}}",
          "range": true,
          "refId": "A",
          "exemplar": true
        }
      ],
      "title": "Blocked Tool Calls",
      "transparent": true,
      "type": "timeseries"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "${metrics_datasource}"
      },
      "description": "This panel displays the current 95th percentile time to first token across GenAI systems. Time to first token is critical for streaming applications and real-time user interactions, representing the delay before content generation begins.",
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "thresholds"
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": 0
              },
              {
                "color": "yellow",
                "value": 2
              },
              {
                "color": "red",
                "value": 5
              }
            ]
          },
          "unit": "s"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 2,
        "w": 12,
        "x": 0,
        "y": 8
      },
      "id": 58,
      "options": {
        "colorMode": "value",
        "graphMode": "area",
        "justifyMode": "auto",
        "orientation": "auto",
        "percentChangeColorMode": "standard",
        "reduceOptions": {
          "calcs": ["lastNotNull"],
          "fields": "",
          "values": false
        },
        "showPercentChange": false,
        "textMode": "auto",
        "wideLayout": true
      },
      "pluginVersion": "12.3.2",
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "${metrics_datasource}"
          },
          "editorMode": "code",
          "expr": "histogram_quantile(0.95, sum by (le, provider) (rate(llm_time_to_first_token_seconds_bucket{agent_name=~\"$agent_name\", provider=~\"$provider\", model=~\"$model\"}[$__rate_interval])))",
          "legendFormat": "{{provider}} P95",
          "range": true,
          "refId": "A"
        }
      ],
      "title": "Time to First Token",
      "type": "stat"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "${metrics_datasource}"
      },
      "description": "Response time distribution across comprehensive token size ranges: 5-16, 17-64, 65-256, 257-1024, 1025-4096 tokens",
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "fillOpacity": 80,
            "gradientMode": "none",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "lineWidth": 1,
            "stacking": {
              "group": "A",
              "mode": "none"
            }
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": 0
              },
              {
                "color": "yellow",
                "value": 5
              },
              {
                "color": "red",
                "value": 15
              }
            ]
          },
          "unit": "s"
        },
        "overrides": [
          {
            "matcher": {
              "id": "byName",
              "options": "5-16 tokens"
            },
            "properties": [
              {
                "id": "color",
                "value": {
                  "fixedColor": "light-green",
                  "mode": "fixed"
                }
              }
            ]
          },
          {
            "matcher": {
              "id": "byName",
              "options": "17-64 tokens"
            },
            "properties": [
              {
                "id": "color",
                "value": {
                  "fixedColor": "light-blue",
                  "mode": "fixed"
                }
              }
            ]
          },
          {
            "matcher": {
              "id": "byName",
              "options": "65-256 tokens"
            },
            "properties": [
              {
                "id": "color",
                "value": {
                  "fixedColor": "yellow",
                  "mode": "fixed"
                }
              }
            ]
          },
          {
            "matcher": {
              "id": "byName",
              "options": "257-1024 tokens"
            },
            "properties": [
              {
                "id": "color",
                "value": {
                  "fixedColor": "orange",
                  "mode": "fixed"
                }
              }
            ]
          },
          {
            "matcher": {
              "id": "byName",
              "options": "1025-4096 tokens"
            },
            "properties": [
              {
                "id": "color",
                "value": {
                  "fixedColor": "red",
                  "mode": "fixed"
                }
              }
            ]
          }
        ]
      },
      "gridPos": {
        "h": 6,
        "w": 12,
        "x": 12,
        "y": 8
      },
      "id": 52,
      "options": {
        "legend": {
          "calcs": [],
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "hideZeros": false,
          "mode": "single",
          "sort": "none"
        }
      },
      "pluginVersion": "12.3.2",
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "${metrics_datasource}"
          },
          "editorMode": "code",
          "expr": "histogram_quantile(0.50, sum(rate(llm_request_duration_seconds_bucket{agent_name=~\"$agent_name\", provider=~\"$provider\", model=~\"$model\"}[$__rate_interval])) by (le)) * on() (sum(rate(llm_token_usage_bucket{le=\"16\", agent_name=~\"$agent_name\", provider=~\"$provider\", model=~\"$model\"}[$__rate_interval])) - sum(rate(llm_token_usage_bucket{le=\"4\", agent_name=~\"$agent_name\", provider=~\"$provider\", model=~\"$model\"}[$__rate_interval])) > 0)",
          "legendFormat": "5-16 tokens",
          "range": true,
          "refId": "A"
        },
        {
          "datasource": {
            "type": "prometheus",
            "uid": "${metrics_datasource}"
          },
          "editorMode": "code",
          "expr": "histogram_quantile(0.50, sum(rate(llm_request_duration_seconds_bucket{agent_name=~\"$agent_name\", provider=~\"$provider\", model=~\"$model\"}[$__rate_interval])) by (le)) * on() (sum(rate(llm_token_usage_bucket{le=\"64\", agent_name=~\"$agent_name\", provider=~\"$provider\", model=~\"$model\"}[$__rate_interval])) - sum(rate(llm_token_usage_bucket{le=\"16\", agent_name=~\"$agent_name\", provider=~\"$provider\", model=~\"$model\"}[$__rate_interval])) > 0)",
          "legendFormat": "17-64 tokens",
          "range": true,
          "refId": "B"
        },
        {
          "datasource": {
            "type": "prometheus",
            "uid": "${metrics_datasource}"
          },
          "editorMode": "code",
          "expr": "histogram_quantile(0.50, sum(rate(llm_request_duration_seconds_bucket{agent_name=~\"$agent_name\", provider=~\"$provider\", model=~\"$model\"}[$__rate_interval])) by (le)) * on() (sum(rate(llm_token_usage_bucket{le=\"256\", agent_name=~\"$agent_name\", provider=~\"$provider\", model=~\"$model\"}[$__rate_interval])) - sum(rate(llm_token_usage_bucket{le=\"64\", agent_name=~\"$agent_name\", provider=~\"$provider\", model=~\"$model\"}[$__rate_interval])) > 0)",
          "legendFormat": "65-256 tokens",
          "range": true,
          "refId": "C"
        },
        {
          "datasource": {
            "type": "prometheus",
            "uid": "${metrics_datasource}"
          },
          "editorMode": "code",
          "expr": "histogram_quantile(0.50, sum(rate(llm_request_duration_seconds_bucket{agent_name=~\"$agent_name\", provider=~\"$provider\", model=~\"$model\"}[$__rate_interval])) by (le)) * on() (sum(rate(llm_token_usage_bucket{le=\"1024\", agent_name=~\"$agent_name\", provider=~\"$provider\", model=~\"$model\"}[$__rate_interval])) - sum(rate(llm_token_usage_bucket{le=\"256\", agent_name=~\"$agent_name\", provider=~\"$provider\", model=~\"$model\"}[$__rate_interval])) > 0)",
          "legendFormat": "257-1024 tokens",
          "range": true,
          "refId": "D"
        },
        {
          "datasource": {
            "type": "prometheus",
            "uid": "${metrics_datasource}"
          },
          "editorMode": "code",
          "expr": "histogram_quantile(0.50, sum(rate(llm_request_duration_seconds_bucket{agent_name=~\"$agent_name\", provider=~\"$provider\", model=~\"$model\"}[$__rate_interval])) by (le)) * on() (sum(rate(llm_token_usage_bucket{le=\"4096\", agent_name=~\"$agent_name\", provider=~\"$provider\", model=~\"$model\"}[$__rate_interval])) - sum(rate(llm_token_usage_bucket{le=\"1024\", agent_name=~\"$agent_name\", provider=~\"$provider\", model=~\"$model\"}[$__rate_interval])) > 0)",
          "legendFormat": "1025-4096 tokens",
          "range": true,
          "refId": "E"
        }
      ],
      "title": "Response Time Distribution by Token Size Ranges",
      "type": "histogram"
    },
    {
      "datasource": {
        "type": "tempo",
        "uid": "${traces_datasource}"
      },
      "description": "This panel displays the distribution of request durations for both GenAI and VectorDB services. It highlights how long requests take to complete, from the shortest to the longest durations, offering insights into system performance and efficiency. Understanding this distribution helps in identifying bottlenecks and optimizing response times for both services.",
      "fieldConfig": {
        "defaults": {
          "color": {
            "fixedColor": "blue",
            "mode": "palette-classic"
          },
          "custom": {
            "fillOpacity": 100,
            "gradientMode": "hue",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "lineWidth": 3,
            "stacking": {
              "group": "A",
              "mode": "none"
            }
          },
          "mappings": [],
          "min": 0,
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": 0
              },
              {
                "color": "red",
                "value": 10
              }
            ]
          },
          "unit": "s"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 4,
        "w": 12,
        "x": 0,
        "y": 10
      },
      "id": 4,
      "options": {
        "legend": {
          "calcs": [],
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": false
        },
        "tooltip": {
          "hideZeros": false,
          "mode": "single",
          "sort": "none"
        }
      },
      "pluginVersion": "12.3.2",
      "targets": [
        {
          "datasource": {
            "type": "tempo",
            "uid": "${traces_datasource}"
          },
          "filters": [
            {
              "id": "status",
              "operator": "=",
              "scope": "intrinsic",
              "tag": "status",
              "value": "ok",
              "valueType": "keyword"
            }
          ],
          "limit": 20,
          "metricsQueryType": "range",
          "query": "{status=ok && resource.service.name=\"Archestra\" && span.route.category=\"llm-proxy\" }",
          "queryType": "traceql",
          "refId": "A",
          "tableType": "traces"
        }
      ],
      "title": "Request Duration Distribution",
      "type": "histogram"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "${metrics_datasource}"
      },
      "description": "This panel shows the request rate breakdown by individual GenAI models over time. It helps identify which models are most frequently used and track demand patterns for specific models, enabling better resource allocation and performance monitoring at the model level.",
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisBorderShow": false,
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "barWidthFactor": 0.6,
            "drawStyle": "bars",
            "fillOpacity": 0,
            "gradientMode": "hue",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "insertNulls": false,
            "lineInterpolation": "linear",
            "lineWidth": 1,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "always",
            "showValues": false,
            "spanNulls": false,
            "stacking": {
              "group": "A",
              "mode": "normal"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": 0
              },
              {
                "color": "red",
                "value": 80
              }
            ]
          },
          "unit": "reqps"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 7,
        "w": 14,
        "x": 0,
        "y": 14
      },
      "id": 28,
      "options": {
        "legend": {
          "calcs": [],
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "hideZeros": false,
          "mode": "multi",
          "sort": "none"
        }
      },
      "pluginVersion": "12.3.2",
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "${metrics_datasource}"
          },
          "editorMode": "code",
          "expr": "sum(rate(llm_request_duration_seconds_count{agent_name=~\"$agent_name\", provider=~\"$provider\", model=~\"$model\"}[$__rate_interval]) ) by (model)",
          "legendFormat": "__auto",
          "range": true,
          "refId": "A",
          "exemplar": true
        }
      ],
      "title": "Request Rate by Model",
      "transparent": true,
      "type": "timeseries"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "${metrics_datasource}"
      },
      "description": "This panel displays the ranking of GenAI models based on their usage frequency. It identifies which models are most popular or in-demand, providing insights into user preferences and operational trends. Analyzing this helps in resource allocation, optimizing model availability, and understanding which GenAI models are driving usage.",
      "fieldConfig": {
        "defaults": {
          "color": {
            "fixedColor": "purple",
            "mode": "shades"
          },
          "mappings": [],
          "min": 0,
          "noValue": "0",
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": 0
              },
              {
                "color": "red",
                "value": 80
              }
            ]
          },
          "unit": "none"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 7,
        "w": 10,
        "x": 14,
        "y": 14
      },
      "id": 62,
      "options": {
        "displayMode": "gradient",
        "legend": {
          "calcs": [],
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": false
        },
        "maxVizHeight": 300,
        "minVizHeight": 16,
        "minVizWidth": 8,
        "namePlacement": "auto",
        "orientation": "horizontal",
        "reduceOptions": {
          "calcs": ["lastNotNull"],
          "fields": "",
          "values": false
        },
        "showUnfilled": true,
        "sizing": "auto",
        "valueMode": "text"
      },
      "pluginVersion": "12.3.2",
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "${metrics_datasource}"
          },
          "editorMode": "code",
          "expr": "topk(5, sum by(model) (llm_request_duration_seconds_count{agent_name=~\"$agent_name\", provider=~\"$provider\", model=~\"$model\"}))",
          "legendFormat": "__auto",
          "range": true,
          "refId": "A"
        }
      ],
      "title": "Top Models by Usage",
      "transparent": true,
      "type": "bargauge"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "${metrics_datasource}"
      },
      "description": "This panel displays a comparative graph showing the average number of tokens consumed for completions and prompts against the average usage cost. It provides a visual representation of the relationship between the volume of data processed (in tokens) and the financial implications of using GenAI services. Analyzing this comparison helps in assessing cost-effectiveness and guiding strategic decisions for efficient resource utilization.",
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisBorderShow": false,
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "barWidthFactor": 0.6,
            "drawStyle": "bars",
            "fillOpacity": 30,
            "gradientMode": "hue",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "insertNulls": false,
            "lineInterpolation": "smooth",
            "lineStyle": {
              "fill": "solid"
            },
            "lineWidth": 2,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "always",
            "showValues": false,
            "spanNulls": true,
            "stacking": {
              "group": "A",
              "mode": "none"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": 0
              },
              {
                "color": "red",
                "value": 80
              }
            ]
          },
          "unit": "none"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 6,
        "w": 24,
        "x": 0,
        "y": 21
      },
      "id": 6,
      "options": {
        "legend": {
          "calcs": [],
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "hideZeros": false,
          "mode": "single",
          "sort": "none"
        }
      },
      "pluginVersion": "12.3.2",
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "${metrics_datasource}"
          },
          "disableTextWrap": false,
          "editorMode": "code",
          "expr": "sum(rate(llm_tokens_total{type=\"input\", agent_name=~\"$agent_name\", provider=~\"$provider\", model=~\"$model\"}[$__rate_interval])) / sum(rate(llm_request_duration_seconds_count{agent_name=~\"$agent_name\", provider=~\"$provider\", model=~\"$model\"}[$__rate_interval]))",
          "fullMetaSearch": false,
          "includeNullMetadata": true,
          "instant": false,
          "legendFormat": "Prompt Tokens",
          "range": true,
          "refId": "A",
          "useBackend": false,
          "exemplar": true
        },
        {
          "datasource": {
            "type": "prometheus",
            "uid": "${metrics_datasource}"
          },
          "disableTextWrap": false,
          "editorMode": "code",
          "expr": "sum(rate(llm_tokens_total{type=\"output\", agent_name=~\"$agent_name\", provider=~\"$provider\", model=~\"$model\"}[$__rate_interval])) / sum(rate(llm_request_duration_seconds_count{agent_name=~\"$agent_name\", provider=~\"$provider\", model=~\"$model\"}[$__rate_interval]))",
          "fullMetaSearch": false,
          "includeNullMetadata": true,
          "instant": false,
          "legendFormat": "Completion Tokens",
          "range": true,
          "refId": "B",
          "useBackend": false,
          "exemplar": true
        },
        {
          "datasource": {
            "type": "prometheus",
            "uid": "${metrics_datasource}"
          },
          "disableTextWrap": false,
          "editorMode": "code",
          "expr": "sum(rate(llm_cost_total{agent_name=~\"$agent_name\", provider=~\"$provider\", model=~\"$model\"}[$__rate_interval])) / sum(rate(llm_request_duration_seconds_count{agent_name=~\"$agent_name\", provider=~\"$provider\", model=~\"$model\"}[$__rate_interval]))",
          "fullMetaSearch": false,
          "includeNullMetadata": true,
          "instant": false,
          "legendFormat": "Usage Cost",
          "range": true,
          "refId": "C",
          "useBackend": false,
          "exemplar": true
        }
      ],
      "title": "Average Token Consumption vs. Average Usage Cost Comparison",
      "transparent": true,
      "type": "timeseries"
    },
    {
      "collapsed": false,
      "gridPos": {
        "h": 1,
        "w": 24,
        "x": 0,
        "y": 27
      },
      "id": 47,
      "panels": [],
      "title": "Traces",
      "type": "row"
    },
    {
      "datasource": {
        "type": "tempo",
        "uid": "${traces_datasource}"
      },
      "description": "This panel shows detailed trace information for GenAI requests, including trace IDs, service names, operation names, and durations. Click on trace IDs to view detailed trace analysis and understand request flow through your system.",
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "thresholds"
          },
          "custom": {
            "align": "auto",
            "cellOptions": {
              "type": "auto"
            },
            "footer": {
              "reducers": []
            },
            "inspect": false,
            "wrapHeaderText": false
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": 0
              },
              {
                "color": "red",
                "value": 80
              }
            ]
          }
        },
        "overrides": [
          {
            "matcher": {
              "id": "byName",
              "options": "Trace ID"
            },
            "properties": [
              {
                "id": "links",
                "value": [
                  {
                    "targetBlank": false,
                    "title": "View trace waterfall",
                    "url": "/d/archestra-genai-observability/archestra-genai-observability?var-traceId=${__value.raw}&var-traces_datasource=${traces_datasource}&var-metrics_datasource=${metrics_datasource}&${__url_time_range}"
                  }
                ]
              }
            ]
          }
        ]
      },
      "gridPos": {
        "h": 21,
        "w": 11,
        "x": 0,
        "y": 28
      },
      "id": 44,
      "options": {
        "cellHeight": "sm",
        "frameIndex": 0,
        "showHeader": true,
        "sortBy": [
          {
            "desc": true,
            "displayName": "Start time"
          }
        ]
      },
      "pluginVersion": "12.3.2",
      "targets": [
        {
          "datasource": {
            "type": "tempo",
            "uid": "${traces_datasource}"
          },
          "limit": 20,
          "metricsQueryType": "range",
          "query": "{resource.service.name=\"Archestra\" && span.route.category=\"llm-proxy\" && span.gen_ai.agent.name=~\"$agent_name\" && span.gen_ai.provider.name=~\"$provider\" && span.gen_ai.request.model=~\"$model\"}",
          "queryType": "traceql",
          "refId": "A",
          "tableType": "traces"
        }
      ],
      "title": "",
      "transformations": [
        {
          "id": "organize",
          "options": {
            "excludeByName": {
              "rootServiceName": true,
              "rootTraceName": true,
              "Service": true,
              "Name": true
            }
          }
        }
      ],
      "transparent": true,
      "type": "table"
    },
    {
      "fieldConfig": {
        "defaults": {},
        "overrides": []
      },
      "gridPos": {
        "h": 3,
        "w": 13,
        "x": 11,
        "y": 28
      },
      "id": 68,
      "options": {
        "code": {
          "language": "plaintext",
          "showLineNumbers": false,
          "showMiniMap": false
        },
        "content": "> To View Traces:\n>  1. Click a **Trace ID** in the table on the left (or paste a trace ID into the **Trace ID** variable above)\n>  2. The trace view will load below",
        "mode": "markdown"
      },
      "pluginVersion": "12.3.2",
      "title": "",
      "transparent": true,
      "type": "text"
    },
    {
      "datasource": {
        "type": "tempo",
        "uid": "${traces_datasource}"
      },
      "description": "This panel displays the selected trace visualization. Use the trace ID variable or click on traces in the table above to view detailed span information, timing, and request flow through your GenAI services.",
      "fieldConfig": {
        "defaults": {},
        "overrides": []
      },
      "gridPos": {
        "h": 18,
        "w": 13,
        "x": 11,
        "y": 31
      },
      "id": 45,
      "options": {
        "spanFilters": {
          "adhocFilters": [],
          "criticalPathOnly": false,
          "matchesOnly": false,
          "serviceNameOperator": "=",
          "spanNameOperator": "=",
          "tags": [
            {
              "id": "85823c53-344",
              "operator": "="
            }
          ]
        }
      },
      "pluginVersion": "12.3.2",
      "targets": [
        {
          "datasource": {
            "type": "tempo",
            "uid": "${traces_datasource}"
          },
          "limit": 20,
          "metricsQueryType": "range",
          "query": "${traceId}",
          "queryType": "traceql",
          "refId": "A",
          "tableType": "traces"
        }
      ],
      "title": "",
      "transparent": true,
      "type": "traces"
    },
    {
      "collapsed": false,
      "gridPos": {
        "h": 1,
        "w": 24,
        "x": 0,
        "y": 49
      },
      "id": 31,
      "panels": [],
      "title": "Cost",
      "type": "row"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "${metrics_datasource}"
      },
      "description": "This panel displays the cumulative cost over time broken down by GenAI system (OpenAI, Anthropic, Cohere, etc.). It shows spending trends and helps identify which platforms are driving costs, enabling better budget planning and cost optimization strategies.",
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisBorderShow": false,
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "barWidthFactor": 0.6,
            "drawStyle": "line",
            "fillOpacity": 0,
            "gradientMode": "none",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "insertNulls": false,
            "lineInterpolation": "linear",
            "lineWidth": 1,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "auto",
            "showValues": false,
            "spanNulls": false,
            "stacking": {
              "group": "A",
              "mode": "none"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "decimals": 2,
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": 0
              },
              {
                "color": "red",
                "value": 80
              }
            ]
          },
          "unit": "currencyUSD"
        },
        "overrides": [
          {
            "matcher": {
              "id": "byName",
              "options": "Total Cost"
            },
            "properties": [
              {
                "id": "custom.lineStyle",
                "value": {
                  "dash": [10, 10],
                  "fill": "dash"
                }
              }
            ]
          }
        ]
      },
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 50
      },
      "id": 29,
      "options": {
        "legend": {
          "calcs": ["median"],
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "hideZeros": false,
          "mode": "multi",
          "sort": "none"
        }
      },
      "pluginVersion": "12.4.0-21914300000",
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "${metrics_datasource}"
          },
          "editorMode": "code",
          "expr": "sum(increase(llm_cost_total{agent_name=~\"$agent_name\", provider=~\"$provider\", model=~\"$model\"} [$__rate_interval])) / 1000",
          "legendFormat": "Total Cost",
          "range": true,
          "refId": "A",
          "exemplar": true
        },
        {
          "datasource": {
            "type": "prometheus",
            "uid": "${metrics_datasource}"
          },
          "editorMode": "code",
          "expr": "sum(increase(llm_cost_total{agent_name=~\"$agent_name\", provider=~\"$provider\", model=~\"$model\"}[$__rate_interval])) by(provider) / 1000",
          "instant": false,
          "legendFormat": "Cost by {{provider}}",
          "range": true,
          "refId": "B",
          "exemplar": true
        }
      ],
      "title": "Total Cost By Provider",
      "transparent": true,
      "type": "timeseries"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "${metrics_datasource}"
      },
      "description": "This panel displays the cost trends over time broken down by individual GenAI models. It helps track spending patterns for specific models and identify which models are driving costs, enabling model-level cost optimization and budget allocation.",
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisBorderShow": false,
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "barWidthFactor": 0.6,
            "drawStyle": "line",
            "fillOpacity": 0,
            "gradientMode": "hue",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "insertNulls": false,
            "lineInterpolation": "linear",
            "lineStyle": {
              "fill": "solid"
            },
            "lineWidth": 1,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "auto",
            "showValues": false,
            "spanNulls": false,
            "stacking": {
              "group": "A",
              "mode": "none"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "decimals": 2,
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": 0
              },
              {
                "color": "red",
                "value": 80
              }
            ]
          },
          "unit": "currencyUSD"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 50
      },
      "id": 32,
      "options": {
        "legend": {
          "calcs": ["median"],
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "hideZeros": false,
          "mode": "multi",
          "sort": "none"
        }
      },
      "pluginVersion": "12.4.0-21914300000",
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "${metrics_datasource}"
          },
          "editorMode": "code",
          "expr": "sum(increase(llm_cost_total{agent_name=~\"$agent_name\", provider=~\"$provider\", model=~\"$model\"}[$__rate_interval])) by(model) / 1000",
          "legendFormat": "__auto",
          "range": true,
          "refId": "A",
          "exemplar": true
        }
      ],
      "title": "Total Cost By Model",
      "transparent": true,
      "type": "timeseries"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "${metrics_datasource}"
      },
      "description": "This panel displays the total cost breakdown by GenAI system as a horizontal bar chart. It shows which platforms (OpenAI, Anthropic, Cohere, etc.) are consuming the most budget, enabling cost analysis and platform comparison for better financial decision making.",
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "continuous-GrYlRd"
          },
          "decimals": 2,
          "mappings": [],
          "min": 0,
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": 0
              },
              {
                "color": "red",
                "value": 80
              }
            ]
          },
          "unit": "currencyUSD"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 58
      },
      "id": 30,
      "options": {
        "displayMode": "lcd",
        "legend": {
          "calcs": [],
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": false
        },
        "maxVizHeight": 300,
        "minVizHeight": 16,
        "minVizWidth": 8,
        "namePlacement": "auto",
        "orientation": "horizontal",
        "reduceOptions": {
          "calcs": ["lastNotNull"],
          "fields": "",
          "values": true
        },
        "showUnfilled": true,
        "sizing": "auto",
        "valueMode": "text"
      },
      "pluginVersion": "12.4.0-21914300000",
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "${metrics_datasource}"
          },
          "disableTextWrap": false,
          "editorMode": "code",
          "exemplar": false,
          "expr": "sum(increase(llm_cost_total{agent_name=~\"$agent_name\", provider=~\"$provider\", model=~\"$model\"}[$__range])) by(provider) / 10000",
          "fullMetaSearch": false,
          "includeNullMetadata": true,
          "instant": true,
          "legendFormat": "{{provider}}",
          "range": false,
          "refId": "A",
          "useBackend": false
        }
      ],
      "title": "Total Cost By Provider",
      "transformations": [
        {
          "id": "seriesToRows",
          "options": {}
        },
        {
          "id": "sortBy",
          "options": {
            "fields": {},
            "sort": [
              {
                "desc": true,
                "field": "Value"
              }
            ]
          }
        }
      ],
      "transparent": true,
      "type": "bargauge"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "${metrics_datasource}"
      },
      "description": "This panel shows the total cost breakdown by model as a horizontal bar chart. It provides a clear view of which GenAI models consume the most budget, helping prioritize cost optimization efforts and model selection strategies.",
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "continuous-GrYlRd"
          },
          "decimals": 2,
          "mappings": [],
          "min": 0,
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": 0
              },
              {
                "color": "red",
                "value": 80
              }
            ]
          },
          "unit": "currencyUSD"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 58
      },
      "id": 33,
      "options": {
        "displayMode": "lcd",
        "legend": {
          "calcs": [],
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": false
        },
        "maxVizHeight": 300,
        "minVizHeight": 16,
        "minVizWidth": 8,
        "namePlacement": "auto",
        "orientation": "horizontal",
        "reduceOptions": {
          "calcs": ["lastNotNull"],
          "fields": "",
          "values": true
        },
        "showUnfilled": true,
        "sizing": "auto",
        "valueMode": "color"
      },
      "pluginVersion": "12.4.0-21914300000",
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "${metrics_datasource}"
          },
          "disableTextWrap": false,
          "editorMode": "code",
          "exemplar": false,
          "expr": "sum(increase(llm_cost_total{agent_name=~\"$agent_name\", provider=~\"$provider\", model=~\"$model\"}[$__range])) by(model) / 10000",
          "fullMetaSearch": false,
          "includeNullMetadata": true,
          "instant": true,
          "legendFormat": "__auto",
          "range": false,
          "refId": "A",
          "useBackend": false
        }
      ],
      "title": "Total Cost By Model",
      "transformations": [
        {
          "id": "seriesToRows",
          "options": {}
        },
        {
          "id": "sortBy",
          "options": {
            "fields": {},
            "sort": [
              {
                "desc": true,
                "field": "Value"
              }
            ]
          }
        }
      ],
      "transparent": true,
      "type": "bargauge"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "${metrics_datasource}"
      },
      "description": "This panel displays the distribution of GenAI requests across different platforms such as OpenAI, Cohere, Anthropic, etc. It shows where requests are being sent, giving insights into platform popularity and usage patterns. Monitoring this helps in understanding platform preferences, and it can guide strategic decisions for integration or diversification of GenAI services.",
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            }
          },
          "mappings": [],
          "min": 0,
          "unit": "none"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 5,
        "w": 8,
        "x": 8,
        "y": 66
      },
      "id": 16,
      "options": {
        "legend": {
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": false
        },
        "pieType": "pie",
        "reduceOptions": {
          "calcs": ["lastNotNull"],
          "fields": "",
          "values": false
        },
        "sort": "desc",
        "tooltip": {
          "hideZeros": false,
          "mode": "single",
          "sort": "none"
        }
      },
      "pluginVersion": "12.4.0-21914300000",
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "${metrics_datasource}"
          },
          "disableTextWrap": false,
          "editorMode": "code",
          "expr": "sum by(provider) (llm_request_duration_seconds_count{agent_name=~\"$agent_name\", provider=~\"$provider\", model=~\"$model\"})",
          "fullMetaSearch": false,
          "includeNullMetadata": true,
          "instant": false,
          "legendFormat": "__auto",
          "range": true,
          "refId": "A",
          "useBackend": false
        }
      ],
      "title": "Requests by Provider",
      "transparent": true,
      "type": "piechart"
    },
    {
      "collapsed": false,
      "gridPos": {
        "h": 1,
        "w": 24,
        "x": 0,
        "y": 71
      },
      "id": 64,
      "panels": [],
      "title": "External Agent Executions",
      "type": "row"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "${metrics_datasource}"
      },
      "description": "Total number of unique agent executions within the selected time range, grouped by agent name. An execution is identified by the X-Archestra-Execution-Id header.",
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": 0
              }
            ]
          },
          "unit": "short"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 6,
        "w": 24,
        "x": 0,
        "y": 72
      },
      "id": 65,
      "options": {
        "colorMode": "value",
        "graphMode": "area",
        "justifyMode": "auto",
        "orientation": "auto",
        "reduceOptions": {
          "calcs": ["lastNotNull"],
          "fields": "",
          "values": false
        },
        "textMode": "auto"
      },
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "${metrics_datasource}"
          },
          "editorMode": "code",
          "expr": "round(sum by (external_agent_id) (sum_over_time(increase(agent_executions_total{external_agent_id!=\"\"}[1m])[$__range:1m])))",
          "instant": true,
          "legendFormat": "{{external_agent_id}}",
          "queryType": "instant",
          "range": false,
          "refId": "A"
        }
      ],
      "title": "Executions per Agent",
      "type": "stat"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "${metrics_datasource}"
      },
      "description": "Running total of LLM API spending per agent. Each line shows accumulated cost over time \u2014 the value at the right edge is the total for the selected period.",
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisBorderShow": false,
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "barWidthFactor": 0.6,
            "drawStyle": "line",
            "fillOpacity": 10,
            "gradientMode": "none",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "insertNulls": false,
            "lineInterpolation": "linear",
            "lineWidth": 1,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "auto",
            "showValues": false,
            "spanNulls": false,
            "stacking": {
              "group": "A",
              "mode": "none"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": 0
              }
            ]
          },
          "unit": "currencyUSD"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 78
      },
      "id": 66,
      "options": {
        "legend": {
          "calcs": ["lastNotNull"],
          "displayMode": "table",
          "placement": "right",
          "showLegend": true
        },
        "tooltip": {
          "hideZeros": false,
          "mode": "multi",
          "sort": "none"
        }
      },
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "${metrics_datasource}"
          },
          "editorMode": "code",
          "expr": "sum by (external_agent_id) (sum_over_time(increase(llm_cost_total{external_agent_id!=\"\"}[1m])[$__range:1m]))",
          "instant": false,
          "legendFormat": "{{external_agent_id}}",
          "queryType": "range",
          "range": true,
          "refId": "A",
          "exemplar": true
        }
      ],
      "title": "Inference Cost per Agent",
      "type": "timeseries"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "${metrics_datasource}"
      },
      "description": "Running total of full operating cost per agent: LLM API spending + a fixed fee per execution (default: $1, configurable in the query). The value at the right edge is the total for the selected period.",
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisBorderShow": false,
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "barWidthFactor": 0.6,
            "drawStyle": "line",
            "fillOpacity": 10,
            "gradientMode": "none",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "insertNulls": false,
            "lineInterpolation": "linear",
            "lineWidth": 1,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "auto",
            "showValues": false,
            "spanNulls": false,
            "stacking": {
              "group": "A",
              "mode": "none"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": 0
              }
            ]
          },
          "unit": "currencyUSD"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 78
      },
      "id": 67,
      "options": {
        "legend": {
          "calcs": ["lastNotNull"],
          "displayMode": "table",
          "placement": "right",
          "showLegend": true
        },
        "tooltip": {
          "hideZeros": false,
          "mode": "multi",
          "sort": "none"
        }
      },
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "${metrics_datasource}"
          },
          "editorMode": "code",
          "expr": "sum by (external_agent_id) (sum_over_time(increase(llm_cost_total{external_agent_id!=\"\"}[1m])[$__range:1m])) + round(sum by (external_agent_id) (sum_over_time(increase(agent_executions_total{external_agent_id!=\"\"}[1m])[$__range:1m]))) * 1",
          "instant": false,
          "legendFormat": "{{external_agent_id}}",
          "queryType": "range",
          "range": true,
          "refId": "A",
          "exemplar": true
        }
      ],
      "title": "Inference + Execution Cost",
      "type": "timeseries"
    },
    {
      "collapsed": false,
      "gridPos": {
        "h": 1,
        "w": 24,
        "x": 0,
        "y": 86
      },
      "id": 46,
      "panels": [],
      "title": "Latency",
      "type": "row"
    },
    {
      "description": "This panel displays section header for system-level latency analysis. It provides visual separation between system-level and model-level latency breakdowns.",
      "fieldConfig": {
        "defaults": {},
        "overrides": []
      },
      "gridPos": {
        "h": 2,
        "w": 12,
        "x": 0,
        "y": 87
      },
      "id": 59,
      "options": {
        "code": {
          "language": "plaintext",
          "showLineNumbers": false,
          "showMiniMap": false
        },
        "content": "### By System",
        "mode": "markdown"
      },
      "pluginVersion": "12.4.0-21914300000",
      "title": "",
      "type": "text"
    },
    {
      "description": "This panel displays section header for model-level latency analysis. It provides visual separation between system-level and model-level latency breakdowns.",
      "fieldConfig": {
        "defaults": {},
        "overrides": []
      },
      "gridPos": {
        "h": 2,
        "w": 12,
        "x": 12,
        "y": 87
      },
      "id": 60,
      "options": {
        "code": {
          "language": "plaintext",
          "showLineNumbers": false,
          "showMiniMap": false
        },
        "content": "### By Model",
        "mode": "markdown"
      },
      "pluginVersion": "12.4.0-21914300000",
      "title": "",
      "type": "text"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "${metrics_datasource}"
      },
      "description": "P95 request latency over time by provider. Shows how long requests take to complete across different LLM platforms.",
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisBorderShow": false,
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "barWidthFactor": 0.6,
            "drawStyle": "line",
            "fillOpacity": 10,
            "gradientMode": "none",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "insertNulls": false,
            "lineInterpolation": "smooth",
            "lineWidth": 2,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "never",
            "spanNulls": false,
            "stacking": {
              "group": "A",
              "mode": "none"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "unit": "s"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 89
      },
      "id": 57,
      "options": {
        "legend": {
          "calcs": ["lastNotNull"],
          "displayMode": "table",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "mode": "multi",
          "sort": "desc"
        }
      },
      "pluginVersion": "12.4.0-21914300000",
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "${metrics_datasource}"
          },
          "expr": "histogram_quantile(0.95, sum(rate(llm_request_duration_seconds_bucket{agent_name=~\"$agent_name\", provider=~\"$provider\", model=~\"$model\"}[$__rate_interval])) by (le, provider))",
          "legendFormat": "{{provider}}",
          "range": true,
          "refId": "A",
          "exemplar": true
        }
      ],
      "title": "Latency P95 by Provider",
      "transparent": true,
      "type": "timeseries"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "${metrics_datasource}"
      },
      "description": "P95 request latency over time by model. Shows how long requests take to complete across different LLM models.",
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisBorderShow": false,
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "barWidthFactor": 0.6,
            "drawStyle": "line",
            "fillOpacity": 10,
            "gradientMode": "none",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "insertNulls": false,
            "lineInterpolation": "smooth",
            "lineWidth": 2,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "never",
            "spanNulls": false,
            "stacking": {
              "group": "A",
              "mode": "none"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "unit": "s"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 89
      },
      "id": 53,
      "options": {
        "legend": {
          "calcs": ["lastNotNull"],
          "displayMode": "table",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "mode": "multi",
          "sort": "desc"
        }
      },
      "pluginVersion": "12.4.0-21914300000",
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "${metrics_datasource}"
          },
          "expr": "histogram_quantile(0.95, sum(rate(llm_request_duration_seconds_bucket{agent_name=~\"$agent_name\", provider=~\"$provider\", model=~\"$model\"}[$__rate_interval])) by (le, model))",
          "legendFormat": "{{model}}",
          "range": true,
          "refId": "A",
          "exemplar": true
        }
      ],
      "title": "Latency P95 by Model",
      "transparent": true,
      "type": "timeseries"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "${metrics_datasource}"
      },
      "description": "This panel displays the 95th percentile token generation time grouped by token count ranges and systems. It shows performance patterns across different system capabilities and token volumes, helping understand how different platforms handle various workload sizes.",
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "continuous-GrYlRd"
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": 0
              },
              {
                "color": "red",
                "value": 80
              }
            ]
          }
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 97
      },
      "id": 40,
      "options": {
        "barRadius": 0.1,
        "barWidth": 0.8,
        "groupWidth": 0.7,
        "legend": {
          "calcs": [],
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": true
        },
        "orientation": "horizontal",
        "showValue": "auto",
        "stacking": "none",
        "tooltip": {
          "mode": "multi",
          "sort": "desc"
        },
        "xTickLabelRotation": 0
      },
      "pluginVersion": "12.4.0-21914300000",
      "targets": [
        {
          "editorMode": "code",
          "exemplar": true,
          "expr": "histogram_quantile(0.95, \n  sum by(le, token_count_group, provider) (\n    increase(llm_request_duration_seconds_bucket{agent_name=~\"$agent_name\", provider=~\"$provider\", model=~\"$model\"}[$__range])\n  )\n)",
          "format": "table",
          "instant": true,
          "legendFormat": "__auto",
          "range": false,
          "refId": "A"
        }
      ],
      "title": "Token Generation time by token count",
      "transparent": true,
      "type": "barchart"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "${metrics_datasource}"
      },
      "description": "This panel displays the 95th percentile token generation time grouped by model. It shows performance patterns across different models, helping identify performance bottlenecks and optimize model selection based on throughput requirements.",
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "continuous-GrYlRd"
          },
          "decimals": 2,
          "mappings": [],
          "min": 0,
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": 0
              }
            ]
          },
          "unit": "s"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 97
      },
      "id": 36,
      "options": {
        "displayMode": "lcd",
        "legend": {
          "calcs": [],
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": false
        },
        "maxVizHeight": 300,
        "minVizHeight": 16,
        "namePlacement": "auto",
        "orientation": "horizontal",
        "reduceOptions": {
          "calcs": ["lastNotNull"],
          "fields": "",
          "values": false
        },
        "showUnfilled": true,
        "sizing": "auto",
        "valueMode": "color"
      },
      "targets": [
        {
          "editorMode": "code",
          "exemplar": true,
          "expr": "histogram_quantile(0.95, \n  sum by(le, model) (\n    increase(llm_request_duration_seconds_bucket{agent_name=~\"$agent_name\", provider=~\"$provider\", model=~\"$model\"}[$__range])\n  )\n)",
          "format": "table",
          "instant": true,
          "legendFormat": "{{model}}",
          "range": false,
          "refId": "A"
        }
      ],
      "title": "P95 Token Generation Time by Model",
      "transparent": true,
      "type": "barchart"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "${metrics_datasource}"
      },
      "description": "This panel displays the 95th percentile token generation time by GenAI system as a horizontal bar chart. It provides platform-level performance comparison, helping identify which systems deliver the best response times.",
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "continuous-GrYlRd"
          },
          "decimals": 2,
          "mappings": [],
          "min": 0,
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": 0
              },
              {
                "color": "red",
                "value": 80
              }
            ]
          },
          "unit": "s"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 105
      },
      "id": 38,
      "options": {
        "displayMode": "lcd",
        "legend": {
          "calcs": [],
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": false
        },
        "maxVizHeight": 300,
        "minVizHeight": 16,
        "minVizWidth": 8,
        "namePlacement": "auto",
        "orientation": "horizontal",
        "reduceOptions": {
          "calcs": ["lastNotNull"],
          "fields": "",
          "values": true
        },
        "showUnfilled": true,
        "sizing": "auto",
        "valueMode": "color"
      },
      "pluginVersion": "12.4.0-21914300000",
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "${metrics_datasource}"
          },
          "disableTextWrap": false,
          "editorMode": "code",
          "exemplar": true,
          "expr": "histogram_quantile(0.95, \n  sum by(le, provider) (\n    increase(llm_request_duration_seconds_bucket{agent_name=~\"$agent_name\", provider=~\"$provider\", model=~\"$model\"}[$__range])\n  )\n)",
          "fullMetaSearch": false,
          "includeNullMetadata": true,
          "instant": true,
          "legendFormat": "__auto",
          "range": false,
          "refId": "A",
          "useBackend": false
        }
      ],
      "title": "Tokens Generation Duration (P95)",
      "transformations": [
        {
          "id": "seriesToRows",
          "options": {}
        },
        {
          "id": "sortBy",
          "options": {
            "fields": {},
            "sort": [
              {
                "desc": true,
                "field": "Value"
              }
            ]
          }
        }
      ],
      "transparent": true,
      "type": "bargauge"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "${metrics_datasource}"
      },
      "description": "This panel shows the 95th percentile token generation time by model as a horizontal bar chart. It provides a clear ranking of model performance, helping identify the fastest and slowest models for response time optimization.",
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "continuous-GrYlRd"
          },
          "decimals": 2,
          "mappings": [],
          "min": 0,
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": 0
              },
              {
                "color": "red",
                "value": 80
              }
            ]
          },
          "unit": "s"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 105
      },
      "id": 37,
      "options": {
        "displayMode": "lcd",
        "legend": {
          "calcs": [],
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": false
        },
        "maxVizHeight": 300,
        "minVizHeight": 16,
        "minVizWidth": 8,
        "namePlacement": "auto",
        "orientation": "horizontal",
        "reduceOptions": {
          "calcs": ["lastNotNull"],
          "fields": "",
          "values": true
        },
        "showUnfilled": true,
        "sizing": "auto",
        "valueMode": "color"
      },
      "pluginVersion": "12.4.0-21914300000",
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "${metrics_datasource}"
          },
          "disableTextWrap": false,
          "editorMode": "code",
          "exemplar": true,
          "expr": "histogram_quantile(0.95, \n  sum by(le, model) (\n    increase(llm_request_duration_seconds_bucket{agent_name=~\"$agent_name\", provider=~\"$provider\", model=~\"$model\"}[$__range])\n  )\n)",
          "fullMetaSearch": false,
          "includeNullMetadata": true,
          "instant": true,
          "legendFormat": "__auto",
          "range": false,
          "refId": "A",
          "useBackend": false
        }
      ],
      "title": "Tokens Generation Duration (P95)",
      "transformations": [
        {
          "id": "seriesToRows",
          "options": {}
        },
        {
          "id": "sortBy",
          "options": {
            "fields": {},
            "sort": [
              {
                "desc": true,
                "field": "Value"
              }
            ]
          }
        }
      ],
      "transparent": true,
      "type": "bargauge"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "${metrics_datasource}"
      },
      "description": "This panel displays the 95th percentile time to first token by GenAI system over time. Time to first token is critical for streaming applications and user experience, as it represents the initial response latency before content generation begins.",
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisBorderShow": false,
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "barWidthFactor": 0.6,
            "drawStyle": "line",
            "fillOpacity": 0,
            "gradientMode": "none",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "insertNulls": false,
            "lineInterpolation": "linear",
            "lineWidth": 1,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "auto",
            "showValues": false,
            "spanNulls": false,
            "stacking": {
              "group": "A",
              "mode": "none"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": 0
              },
              {
                "color": "red",
                "value": 80
              }
            ]
          },
          "unit": "s"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 113
      },
      "id": 41,
      "options": {
        "legend": {
          "calcs": ["median"],
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "hideZeros": false,
          "mode": "multi",
          "sort": "none"
        }
      },
      "pluginVersion": "12.4.0-21914300000",
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "${metrics_datasource}"
          },
          "editorMode": "code",
          "expr": "histogram_quantile(0.95, sum(rate(llm_time_to_first_token_seconds_bucket{agent_name=~\"$agent_name\", provider=~\"$provider\", model=~\"$model\"}[$__rate_interval])) by (provider, le))",
          "legendFormat": "__auto",
          "range": true,
          "refId": "A",
          "exemplar": true
        }
      ],
      "title": "Time to first token",
      "transparent": true,
      "type": "timeseries"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "${metrics_datasource}"
      },
      "description": "This panel shows the 95th percentile time to first token by model over time. It helps identify which models provide the fastest initial response for streaming applications and real-time user interactions.",
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisBorderShow": false,
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "barWidthFactor": 0.6,
            "drawStyle": "line",
            "fillOpacity": 0,
            "gradientMode": "none",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "insertNulls": false,
            "lineInterpolation": "linear",
            "lineWidth": 1,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "auto",
            "showValues": false,
            "spanNulls": false,
            "stacking": {
              "group": "A",
              "mode": "none"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": 0
              },
              {
                "color": "red",
                "value": 80
              }
            ]
          },
          "unit": "s"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 113
      },
      "id": 42,
      "options": {
        "legend": {
          "calcs": ["median"],
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "hideZeros": false,
          "mode": "multi",
          "sort": "none"
        }
      },
      "pluginVersion": "12.4.0-21914300000",
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "${metrics_datasource}"
          },
          "editorMode": "code",
          "expr": "histogram_quantile(0.95, sum(rate(llm_time_to_first_token_seconds_bucket{agent_name=~\"$agent_name\", provider=~\"$provider\", model=~\"$model\"}[$__rate_interval])) by (model, le))",
          "legendFormat": "__auto",
          "range": true,
          "refId": "A",
          "exemplar": true
        }
      ],
      "title": "Time to first token",
      "transparent": true,
      "type": "timeseries"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "${metrics_datasource}"
      },
      "description": "This panel displays the 95th percentile token generation time by GenAI system over time. It shows performance trends and helps monitor latency patterns across different platforms, enabling proactive performance management and system optimization.",
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisBorderShow": false,
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "barWidthFactor": 0.6,
            "drawStyle": "line",
            "fillOpacity": 0,
            "gradientMode": "none",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "insertNulls": false,
            "lineInterpolation": "linear",
            "lineWidth": 1,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "auto",
            "showValues": false,
            "spanNulls": false,
            "stacking": {
              "group": "A",
              "mode": "none"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": 0
              },
              {
                "color": "red",
                "value": 80
              }
            ]
          },
          "unit": "s"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 9,
        "w": 12,
        "x": 0,
        "y": 121
      },
      "id": 39,
      "options": {
        "legend": {
          "calcs": ["median"],
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "hideZeros": false,
          "mode": "multi",
          "sort": "none"
        }
      },
      "pluginVersion": "12.4.0-21914300000",
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "${metrics_datasource}"
          },
          "editorMode": "code",
          "expr": "histogram_quantile(0.95, sum(rate(llm_request_duration_seconds_bucket{agent_name=~\"$agent_name\", provider=~\"$provider\", model=~\"$model\"}[$__rate_interval])) by (provider, le))",
          "legendFormat": "__auto",
          "range": true,
          "refId": "A",
          "exemplar": true
        }
      ],
      "title": "Tokens Generation Duration",
      "transparent": true,
      "type": "timeseries"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "${metrics_datasource}"
      },
      "description": "This panel displays the 95th percentile token generation time by model over time. It provides detailed performance tracking for each model, enabling model-specific performance optimization and selection based on latency requirements.",
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisBorderShow": false,
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "barWidthFactor": 0.6,
            "drawStyle": "line",
            "fillOpacity": 0,
            "gradientMode": "none",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "insertNulls": false,
            "lineInterpolation": "linear",
            "lineWidth": 1,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "auto",
            "showValues": false,
            "spanNulls": false,
            "stacking": {
              "group": "A",
              "mode": "none"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": 0
              },
              {
                "color": "red",
                "value": 80
              }
            ]
          },
          "unit": "s"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 9,
        "w": 12,
        "x": 12,
        "y": 121
      },
      "id": 43,
      "options": {
        "legend": {
          "calcs": ["median"],
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "hideZeros": false,
          "mode": "single",
          "sort": "none"
        }
      },
      "pluginVersion": "12.4.0-21914300000",
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "${metrics_datasource}"
          },
          "editorMode": "code",
          "expr": "histogram_quantile(0.95, sum(rate(llm_request_duration_seconds_bucket{agent_name=~\"$agent_name\", provider=~\"$provider\", model=~\"$model\"}[$__rate_interval])) by (model, le))",
          "legendFormat": "__auto",
          "range": true,
          "refId": "A",
          "exemplar": true
        }
      ],
      "title": "Tokens Generation Duration",
      "transparent": true,
      "type": "timeseries"
    }
  ],
  "preload": false,
  "refresh": "30s",
  "schemaVersion": 42,
  "tags": ["archestra", "genai"],
  "templating": {
    "list": [
      {
        "current": {
          "text": "grafanacloud-archestra-traces",
          "value": "grafanacloud-traces"
        },
        "includeAll": false,
        "label": "Traces data source",
        "name": "traces_datasource",
        "options": [],
        "query": "tempo",
        "refresh": 1,
        "regex": "",
        "type": "datasource"
      },
      {
        "current": {
          "text": "grafanacloud-archestra-prom",
          "value": "grafanacloud-prom"
        },
        "includeAll": false,
        "label": "Metrics data source",
        "name": "metrics_datasource",
        "options": [],
        "query": "prometheus",
        "refresh": 1,
        "regex": "(?!grafanacloud-usage|grafanacloud-ml-metrics).+",
        "type": "datasource"
      },
      {
        "allValue": ".*",
        "current": {
          "text": "All",
          "value": "$__all"
        },
        "datasource": {
          "type": "prometheus",
          "uid": "${metrics_datasource}"
        },
        "definition": "label_values(llm_tokens_total, agent_name)",
        "includeAll": true,
        "label": "Agent Name",
        "multi": true,
        "name": "agent_name",
        "options": [],
        "query": {
          "qryType": 1,
          "query": "label_values(llm_tokens_total, agent_name)",
          "refId": "PrometheusVariableQueryEditor-VariableQuery"
        },
        "refresh": 1,
        "regex": "",
        "sort": 1,
        "type": "query"
      },
      {
        "allValue": ".*",
        "current": {
          "text": "All",
          "value": "$__all"
        },
        "datasource": {
          "type": "prometheus",
          "uid": "${metrics_datasource}"
        },
        "definition": "label_values(llm_tokens_total{agent_name=~\"$agent_name\"}, provider)",
        "includeAll": true,
        "label": "Provider",
        "multi": true,
        "name": "provider",
        "options": [],
        "query": {
          "qryType": 1,
          "query": "label_values(llm_tokens_total{agent_name=~\"$agent_name\"}, provider)",
          "refId": "PrometheusVariableQueryEditor-VariableQuery"
        },
        "refresh": 1,
        "regex": "",
        "sort": 1,
        "type": "query"
      },
      {
        "allValue": ".*",
        "current": {
          "text": "All",
          "value": "$__all"
        },
        "datasource": {
          "type": "prometheus",
          "uid": "${metrics_datasource}"
        },
        "definition": "label_values(llm_tokens_total{agent_name=~\"$agent_name\", provider=~\"$provider\"}, model)",
        "includeAll": true,
        "label": "Model",
        "multi": true,
        "name": "model",
        "options": [],
        "query": {
          "qryType": 1,
          "query": "label_values(llm_tokens_total{agent_name=~\"$agent_name\", provider=~\"$provider\"}, model)",
          "refId": "PrometheusVariableQueryEditor-VariableQuery"
        },
        "refresh": 1,
        "regex": "",
        "sort": 1,
        "type": "query"
      },
      {
        "current": {
          "text": "",
          "value": ""
        },
        "label": "Trace ID",
        "name": "traceId",
        "options": [],
        "query": "",
        "type": "textbox"
      }
    ]
  },
  "time": {
    "from": "now-30m",
    "to": "now"
  },
  "timepicker": {},
  "timezone": "browser",
  "title": "Archestra GenAI Observability",
  "uid": "archestra-genai-observability",
  "version": 32
}
